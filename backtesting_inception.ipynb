{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import ta\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import torch.nn as nn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and preprocess data\n",
        "def load_data(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    # df['Datetime'] = pd.to_datetime(df['Date'] + ' ' + df['Time'])\n",
        "    df['Datetime'] = pd.to_datetime(df['Date'])\n",
        "    # df = df.sort_values('Datetime').drop(['Date', 'Time'], axis=1)\n",
        "    df = df.sort_values('Datetime').drop(['Date'], axis=1)\n",
        "    return df\n",
        "\n",
        "def add_technical_indicators(df):\n",
        "    \"\"\"Add technical indicators.\"\"\"\n",
        "    print(\"Adding technical indicators...\")\n",
        "    # Momentum\n",
        "    df['RSI'] = ta.momentum.RSIIndicator(df['Close']).rsi()\n",
        "    df['Momentum'] = ta.momentum.ROCIndicator(df['Close']).roc()\n",
        "    df['CMO'] = ta.momentum.kama(df['Close'])\n",
        "    df['Williams_%R'] = ta.momentum.WilliamsRIndicator(df['High'], df['Low'], df['Close']).williams_r()\n",
        "    # Volatility\n",
        "    df['ATR'] = ta.volatility.AverageTrueRange(df['High'], df['Low'], df['Close']).average_true_range()\n",
        "    bb = ta.volatility.BollingerBands(df['Close'])\n",
        "    df['BB_Mid'] = bb.bollinger_mavg()\n",
        "    df['BB_Upper'] = bb.bollinger_hband()\n",
        "    df['BB_Lower'] = bb.bollinger_lband()\n",
        "    df['BB_Bandwidth'] = bb.bollinger_wband()\n",
        "    keltner = ta.volatility.KeltnerChannel(df['High'], df['Low'], df['Close'])\n",
        "    df['KC_High'] = keltner.keltner_channel_hband()\n",
        "    df['KC_Low'] = keltner.keltner_channel_lband()\n",
        "    donchian = ta.volatility.DonchianChannel(df['High'], df['Low'], df['Close'])\n",
        "    df['DC_High'] = donchian.donchian_channel_hband()\n",
        "    df['DC_Low'] = donchian.donchian_channel_lband()\n",
        "    # Trend\n",
        "    df['SMA_20'] = ta.trend.SMAIndicator(df['Close'], window=20).sma_indicator()\n",
        "    df['EMA_20'] = ta.trend.EMAIndicator(df['Close'], window=20).ema_indicator()\n",
        "    df['DPO'] = ta.trend.DPOIndicator(df['Close']).dpo()\n",
        "    macd = ta.trend.MACD(df['Close'])\n",
        "    df['MACD'] = macd.macd()\n",
        "    df['MACD_Hist'] = macd.macd_diff()\n",
        "    df['Mass_Index'] = ta.trend.mass_index(df['High'], df['Low'])\n",
        "    # Volume\n",
        "    df['AD'] = ta.volume.AccDistIndexIndicator(df['High'], df['Low'], df['Close'], df['Volume']).acc_dist_index()\n",
        "    df['CMF'] = ta.volume.ChaikinMoneyFlowIndicator(df['High'], df['Low'], df['Close'], df['Volume']).chaikin_money_flow()\n",
        "    df['Force_Index'] = ta.volume.ForceIndexIndicator(df['Close'], df['Volume']).force_index()\n",
        "    df['MFI'] = ta.volume.MFIIndicator(df['High'], df['Low'], df['Close'], df['Volume']).money_flow_index()\n",
        "    df['OBV'] = ta.volume.OnBalanceVolumeIndicator(df['Close'], df['Volume']).on_balance_volume()\n",
        "\n",
        "    print(\"Technical indicators added.\")\n",
        "    return df.reset_index(drop=True)\n",
        "\n",
        "def add_basic_features(df):\n",
        "    \"\"\"Final preprocessing: pct_change, time features.\"\"\"\n",
        "    print(\"Applying preprocessing...\")\n",
        "    # Calculate percentage change for OHLC\n",
        "    cols_to_pct = ['Open', 'High', 'Low', 'Close']\n",
        "    existing_cols = [col for col in cols_to_pct if col in df.columns]\n",
        "    if existing_cols:\n",
        "        print(f\"Calculating percentage change for: {existing_cols}\")\n",
        "        df[existing_cols] = df[existing_cols].pct_change().fillna(0) * 100\n",
        "        df['Cum_Return'] = df['Close'].rolling(window=20).sum()\n",
        "        df['Cum_Turnover'] = df['Volume'].rolling(window=20).sum()\n",
        "\n",
        "    # Add time features\n",
        "    if 'Datetime' in df.columns:\n",
        "        print(\"Adding time features...\")\n",
        "        df['Hour'] = df['Datetime'].dt.hour / 23.0\n",
        "        df['Day_Of_Week'] = df['Datetime'].dt.dayofweek / 6.0\n",
        "        df['Minute_Of_Day'] = (df['Datetime'].dt.hour * 60 + df['Datetime'].dt.minute) / 1439.0\n",
        "\n",
        "    print(\"Preprocessing completed.\")\n",
        "    return df.reset_index(drop=True)\n",
        "\n",
        "def normalize_by_blocks(data, block_size):\n",
        "    print(f\"Applying normalize_by_blocks with block_size={block_size}...\")\n",
        "    if isinstance(data, pd.DataFrame):\n",
        "        columns = data.columns\n",
        "        index = data.index\n",
        "        data_np = data.values.astype('float32')\n",
        "    else:\n",
        "        data_np = data.astype('float32')\n",
        "        columns = None\n",
        "        index = None\n",
        "\n",
        "    result = np.zeros_like(data_np)\n",
        "    num_blocks = 0\n",
        "\n",
        "    for start_idx in range(0, len(data_np), block_size):\n",
        "        end_idx = min(start_idx + block_size, len(data_np))\n",
        "        block = data_np[start_idx:end_idx]\n",
        "\n",
        "        if block.shape[0] > 0:\n",
        "            scaler = StandardScaler()\n",
        "            if block.shape[0] == 1:\n",
        "                normalized_block = block - np.mean(block, axis=0)\n",
        "            else:\n",
        "                std_devs = np.std(block, axis=0)\n",
        "                if np.any(std_devs == 0):\n",
        "                    normalized_block = np.zeros_like(block)\n",
        "                    valid_cols = std_devs != 0\n",
        "                    if np.any(valid_cols):\n",
        "                        scaler.fit(block[:, valid_cols])\n",
        "                        normalized_block[:, valid_cols] = scaler.transform(block[:, valid_cols])\n",
        "                    zero_std_cols = std_devs == 0\n",
        "                    if np.any(zero_std_cols):\n",
        "                        normalized_block[:, zero_std_cols] = block[:, zero_std_cols] - np.mean(block[:, zero_std_cols], axis=0)\n",
        "                else:\n",
        "                    normalized_block = scaler.fit_transform(block)\n",
        "\n",
        "            if np.isnan(normalized_block).any() or np.isinf(normalized_block).any():\n",
        "                normalized_block = np.nan_to_num(normalized_block, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "            result[start_idx:end_idx] = normalized_block\n",
        "            num_blocks += 1\n",
        "\n",
        "    print(f\"normalize_by_blocks completed. Processed {num_blocks} blocks.\")\n",
        "    if columns is not None and index is not None:\n",
        "        return pd.DataFrame(result, columns=columns, index=index)\n",
        "    else:\n",
        "        return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model definition\n",
        "class InceptionModule(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__()\n",
        "        self.branch1 = nn.Conv1d(in_channels, 32, kernel_size=1, padding='same')\n",
        "        self.branch3 = nn.Conv1d(in_channels, 32, kernel_size=3, padding='same')\n",
        "        self.branch5 = nn.Conv1d(in_channels, 32, kernel_size=5, padding='same')\n",
        "        self.branch_pool = nn.Sequential(\n",
        "            nn.MaxPool1d(kernel_size=3, stride=1, padding=1),\n",
        "            nn.Conv1d(in_channels, 32, kernel_size=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.cat([self.branch1(x), self.branch3(x), self.branch5(x), self.branch_pool(x)], dim=1)\n",
        "\n",
        "class Time2Vec(nn.Module):\n",
        "    def __init__(self, time_dim, kernel_dim=32):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(time_dim, 1)\n",
        "        self.periodic = nn.Linear(time_dim, kernel_dim - 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        linear = self.linear(x)\n",
        "        periodic = self.periodic(x)\n",
        "        return torch.cat([linear, periodic], dim=-1)\n",
        "\n",
        "class CrossAttentionFusion(nn.Module):\n",
        "    def __init__(self, cnn_dim, transformer_dim):\n",
        "        super().__init__()\n",
        "        self.query = nn.Linear(cnn_dim, transformer_dim)\n",
        "        self.key = nn.Linear(transformer_dim, transformer_dim)\n",
        "        self.value = nn.Linear(transformer_dim, transformer_dim)\n",
        "        \n",
        "    def forward(self, cnn_features, transformer_features):\n",
        "        Q = self.query(cnn_features).unsqueeze(1)  # [batch, 1, transformer_dim]\n",
        "        K = self.key(transformer_features)         # [batch, seq_len, transformer_dim]\n",
        "        V = self.value(transformer_features)       # [batch, seq_len, transformer_dim]\n",
        "        \n",
        "        attn_scores = (Q @ K.transpose(-2, -1)) / (K.size(-1) ** 0.5)  # [batch, 1, seq_len]\n",
        "        attn_weights = torch.softmax(attn_scores, dim=-1)\n",
        "        \n",
        "        return torch.bmm(attn_weights, V).squeeze(1)  # [batch, transformer_dim]\n",
        "\n",
        "class HighwayNetwork(nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        self.gate = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        \n",
        "    def forward(self, fused, transformer):\n",
        "        g = self.gate(fused)\n",
        "        return g * fused + (1 - g) * transformer\n",
        "\n",
        "class EnhancedHybridModel(nn.Module):\n",
        "    def __init__(self, num_features, time_dim, num_classes=3, d_model=512, nhead=16, dim_feedforward=1024, num_layers=6):\n",
        "        super().__init__()\n",
        "        # 1. InceptionTime Branch\n",
        "        self.inception = nn.Sequential(\n",
        "            InceptionModule(num_features),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(2),\n",
        "            InceptionModule(128),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        \n",
        "        # 2. Transformer Branch\n",
        "        self.time2vec = Time2Vec(time_dim, d_model//2)\n",
        "        self.transformer_proj = nn.Linear(num_features, d_model//2)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, batch_first=True\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        \n",
        "        # 3. Fusion\n",
        "        self.cross_attention = CrossAttentionFusion(128, d_model)\n",
        "        self.highway = HighwayNetwork(d_model)\n",
        "        \n",
        "        # 4. Classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(d_model, 128),\n",
        "            nn.LayerNorm(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, time_feature):\n",
        "        # 1. Inception Path\n",
        "        cnn_features = self.inception(x.permute(0, 2, 1))  # [batch, channels, seq_len//2]\n",
        "        cnn_features = cnn_features.mean(dim=-1)          # [batch, channels=128]\n",
        "        \n",
        "        # 2. Transformer Path\n",
        "        time_embed = self.time2vec(time_feature)  # [batch, seq_len, d_model // 2]\n",
        "        x_proj = self.transformer_proj(x)  # [batch, seq_len, d_model // 2]\n",
        "        combined = torch.cat([time_embed, x_proj], dim=-1) # [batch, seq_len, d_model]\n",
        "        transformer_features = self.transformer(combined)  # [batch, seq_len, d_model]\n",
        "        \n",
        "        # 3. Fusion\n",
        "        fused = self.cross_attention(cnn_features, transformer_features)  # [batch, d_model]\n",
        "        output = self.highway(fused, transformer_features.mean(dim=1))   # [batch, d_model]\n",
        "        \n",
        "        return self.classifier(output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BacktestDataset(Dataset):\n",
        "    def __init__(self, df, sequence_length, feature_cols, use_time2vec=True):\n",
        "        self.sequence_length = sequence_length\n",
        "        self.feature_cols = [col for col in feature_cols if col in df.columns]\n",
        "        self.use_time2vec = use_time2vec\n",
        "        self.has_time_input = False\n",
        "\n",
        "        if self.use_time2vec:\n",
        "            if \"Minute_Of_Day\" in df.columns and pd.api.types.is_numeric_dtype(df['Minute_Of_Day']):\n",
        "                self.time_features = df[\"Minute_Of_Day\"].values[:, np.newaxis].astype(np.float32)\n",
        "                self.has_time_input = True\n",
        "            else:\n",
        "                self.time_features = np.zeros((len(df), 1), dtype=np.float32)\n",
        "        else:\n",
        "            self.time_features = np.zeros((len(df), 1), dtype=np.float32)\n",
        "\n",
        "        self.features = df[self.feature_cols].values.astype(np.float32)\n",
        "        self.datetimes = df['Datetime'].values\n",
        "        self.close_prices = df['Close'].values\n",
        "\n",
        "        if len(self.features) <= self.sequence_length:\n",
        "            raise ValueError(f\"DataFrame length ({len(self.features)}) must be greater than sequence_length ({self.sequence_length}).\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features) - self.sequence_length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        end_idx = idx + self.sequence_length\n",
        "        feat_seq = self.features[idx:end_idx]\n",
        "        time_seq = self.time_features[idx:end_idx]\n",
        "        datetime = self.datetimes[end_idx]\n",
        "        close_price = self.close_prices[end_idx]\n",
        "        return torch.from_numpy(feat_seq).float(), torch.from_numpy(time_seq).float(), datetime, close_price\n",
        "\n",
        "class TradingStrategy:\n",
        "    def __init__(self, initial_balance=10000, position_size=0.1, stop_loss=0.02, take_profit=0.04):\n",
        "        self.initial_balance = initial_balance\n",
        "        self.balance = initial_balance\n",
        "        self.position_size = position_size\n",
        "        self.stop_loss = stop_loss\n",
        "        self.take_profit = take_profit\n",
        "        \n",
        "        self.current_position = None\n",
        "        self.entry_price = None\n",
        "        self.trades = []\n",
        "        self.equity_curve = []\n",
        "        \n",
        "    def reset(self):\n",
        "        self.balance = self.initial_balance\n",
        "        self.current_position = None\n",
        "        self.entry_price = None\n",
        "        self.trades = []\n",
        "        self.equity_curve = []\n",
        "        \n",
        "    def open_position(self, position_type, price, datetime):\n",
        "        if self.current_position is not None:\n",
        "            return\n",
        "            \n",
        "        self.current_position = position_type\n",
        "        self.entry_price = price\n",
        "        position_value = self.balance * self.position_size\n",
        "        units = position_value / price\n",
        "        \n",
        "        self.trades.append({\n",
        "            'datetime': datetime,\n",
        "            'type': 'ENTRY',\n",
        "            'position': position_type,\n",
        "            'price': price,\n",
        "            'units': units,\n",
        "            'balance': self.balance\n",
        "        })\n",
        "        \n",
        "    def close_position(self, price, datetime, reason='EXIT'):\n",
        "        if self.current_position is None:\n",
        "            return\n",
        "            \n",
        "        last_trade = self.trades[-1]\n",
        "        units = last_trade['units']\n",
        "        \n",
        "        if self.current_position == 'BUY':\n",
        "            pnl = (price - self.entry_price) * units\n",
        "        else:  # SELL\n",
        "            pnl = (self.entry_price - price) * units\n",
        "            \n",
        "        self.balance += pnl\n",
        "        \n",
        "        self.trades.append({\n",
        "            'datetime': datetime,\n",
        "            'type': reason,\n",
        "            'position': self.current_position,\n",
        "            'price': price,\n",
        "            'units': units,\n",
        "            'pnl': pnl,\n",
        "            'balance': self.balance\n",
        "        })\n",
        "        \n",
        "        self.current_position = None\n",
        "        self.entry_price = None\n",
        "        \n",
        "    def check_stop_loss_take_profit(self, current_price, datetime):\n",
        "        if self.current_position is None:\n",
        "            return\n",
        "            \n",
        "        price_change = (current_price - self.entry_price) / self.entry_price\n",
        "        \n",
        "        if self.current_position == 'BUY':\n",
        "            if price_change <= -self.stop_loss:\n",
        "                self.close_position(current_price, datetime, 'STOP_LOSS')\n",
        "            elif price_change >= self.take_profit:\n",
        "                self.close_position(current_price, datetime, 'TAKE_PROFIT')\n",
        "        else:  # SELL\n",
        "            if price_change >= self.stop_loss:\n",
        "                self.close_position(current_price, datetime, 'STOP_LOSS')\n",
        "            elif price_change <= -self.take_profit:\n",
        "                self.close_position(current_price, datetime, 'TAKE_PROFIT')\n",
        "                \n",
        "    def update_equity(self, current_price, datetime):\n",
        "        equity = self.balance\n",
        "        \n",
        "        if self.current_position is not None:\n",
        "            last_trade = self.trades[-1]\n",
        "            units = last_trade['units']\n",
        "            \n",
        "            if self.current_position == 'BUY':\n",
        "                pnl = (current_price - self.entry_price) * units\n",
        "            else:  # SELL\n",
        "                pnl = (self.entry_price - current_price) * units\n",
        "                \n",
        "            equity += pnl\n",
        "            \n",
        "        self.equity_curve.append({\n",
        "            'datetime': datetime,\n",
        "            'equity': equity\n",
        "        })\n",
        "        \n",
        "    def get_trades_df(self):\n",
        "        return pd.DataFrame(self.trades)\n",
        "    \n",
        "    def get_equity_curve_df(self):\n",
        "        return pd.DataFrame(self.equity_curve)\n",
        "    \n",
        "def run_backtest(model, dataloader, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    \n",
        "    strategy = TradingStrategy()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        loop = tqdm(dataloader, desc='Backtesting')\n",
        "        for features, times, datetime, close_price in loop:\n",
        "            features = features.to(device)\n",
        "            times = times.to(device)\n",
        "            \n",
        "            # Get model prediction\n",
        "            outputs = model(features, times)\n",
        "            probs = torch.softmax(outputs, dim=-1)\n",
        "            prediction = torch.argmax(probs, dim=-1).item()\n",
        "            confidence = probs[0][prediction].item()\n",
        "            \n",
        "            # Check stop loss / take profit for existing position\n",
        "            strategy.check_stop_loss_take_profit(close_price.item(), datetime.item())\n",
        "            \n",
        "            # Open new position if confidence is high enough\n",
        "            if confidence > 0.6:  # Confidence threshold\n",
        "                if prediction == 0 and strategy.current_position is None:  # BUY\n",
        "                    strategy.open_position('BUY', close_price.item(), datetime.item())\n",
        "                elif prediction == 1 and strategy.current_position is None:  # SELL\n",
        "                    strategy.open_position('SELL', close_price.item(), datetime.item())\n",
        "                elif prediction == 2 and strategy.current_position is not None:  # HOLD -> Close position\n",
        "                    strategy.close_position(close_price.item(), datetime.item())\n",
        "            \n",
        "            # Update equity curve\n",
        "            strategy.update_equity(close_price.item(), datetime.item())\n",
        "            \n",
        "    return strategy\n",
        "\n",
        "def plot_equity_curve(equity_df):\n",
        "    plt.figure(figsize=(15, 6))\n",
        "    plt.plot(equity_df['datetime'], equity_df['equity'])\n",
        "    plt.title('Equity Curve')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Equity')\n",
        "    plt.grid(True)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def analyze_trades(trades_df):\n",
        "    if trades_df.empty:\n",
        "        print(\"No trades were executed.\")\n",
        "        return\n",
        "    \n",
        "    # Filter only exit trades\n",
        "    exit_trades = trades_df[trades_df['type'].isin(['EXIT', 'STOP_LOSS', 'TAKE_PROFIT'])]\n",
        "    \n",
        "    # Calculate metrics\n",
        "    total_trades = len(exit_trades)\n",
        "    profitable_trades = len(exit_trades[exit_trades['pnl'] > 0])\n",
        "    win_rate = profitable_trades / total_trades if total_trades > 0 else 0\n",
        "    \n",
        "    total_profit = exit_trades['pnl'].sum()\n",
        "    max_drawdown = min(exit_trades['pnl'])\n",
        "    \n",
        "    avg_profit = exit_trades[exit_trades['pnl'] > 0]['pnl'].mean()\n",
        "    avg_loss = exit_trades[exit_trades['pnl'] < 0]['pnl'].mean()\n",
        "    \n",
        "    profit_factor = abs(exit_trades[exit_trades['pnl'] > 0]['pnl'].sum() / exit_trades[exit_trades['pnl'] < 0]['pnl'].sum()) if len(exit_trades[exit_trades['pnl'] < 0]) > 0 else float('inf')\n",
        "    \n",
        "    # Print results\n",
        "    print(\"\\n=== Trading Performance Analysis ===\")\n",
        "    print(f\"Total Trades: {total_trades}\")\n",
        "    print(f\"Profitable Trades: {profitable_trades}\")\n",
        "    print(f\"Win Rate: {win_rate:.2%}\")\n",
        "    print(f\"Total Profit: ${total_profit:.2f}\")\n",
        "    print(f\"Maximum Drawdown: ${max_drawdown:.2f}\")\n",
        "    print(f\"Average Profit: ${avg_profit:.2f}\")\n",
        "    print(f\"Average Loss: ${avg_loss:.2f}\")\n",
        "    print(f\"Profit Factor: {profit_factor:.2f}\")\n",
        "    \n",
        "    # Plot trade distribution\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hist(exit_trades['pnl'], bins=50)\n",
        "    plt.title('Trade PnL Distribution')\n",
        "    plt.xlabel('Profit/Loss ($)')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Adding technical indicators...\n",
            "Technical indicators added.\n",
            "Applying preprocessing...\n",
            "Calculating percentage change for: ['Open', 'High', 'Low', 'Close']\n",
            "Adding time features...\n",
            "Preprocessing completed.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\longt\\AppData\\Local\\Temp\\ipykernel_19208\\776879440.py:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('save/models/used_model.pth', map_location=DEVICE))\n",
            "Backtesting:   0%|          | 0/1127239 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'numpy.datetime64'>",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     29\u001b[39m model.load_state_dict(torch.load(\u001b[33m'\u001b[39m\u001b[33msave/models/used_model.pth\u001b[39m\u001b[33m'\u001b[39m, map_location=DEVICE))\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Run backtest\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m strategy = \u001b[43mrun_backtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Analyze results\u001b[39;00m\n\u001b[32m     35\u001b[39m trades_df = strategy.get_trades_df()\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 150\u001b[39m, in \u001b[36mrun_backtest\u001b[39m\u001b[34m(model, dataloader, device)\u001b[39m\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m    149\u001b[39m     loop = tqdm(dataloader, desc=\u001b[33m'\u001b[39m\u001b[33mBacktesting\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatetime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_price\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    152\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimes\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\longt\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\std.py:1182\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1179\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1181\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1182\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1185\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\longt\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    627\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    628\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    629\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m630\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    632\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[32m    633\u001b[39m         \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[32m    634\u001b[39m         \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\longt\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    671\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    672\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m673\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    674\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    675\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\longt\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\longt\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:317\u001b[39m, in \u001b[36mdefault_collate\u001b[39m\u001b[34m(batch)\u001b[39m\n\u001b[32m    256\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdefault_collate\u001b[39m(batch):\n\u001b[32m    257\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[33;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[32m    259\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    315\u001b[39m \u001b[33;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[32m    316\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m317\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\longt\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:174\u001b[39m, in \u001b[36mcollate\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    171\u001b[39m transposed = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(*batch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    176\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\longt\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:192\u001b[39m, in \u001b[36mcollate\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    187\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m    188\u001b[39m             \u001b[38;5;66;03m# The sequence type may not support `copy()` / `__setitem__(index, item)`\u001b[39;00m\n\u001b[32m    189\u001b[39m             \u001b[38;5;66;03m# or `__init__(iterable)` (e.g., `range`).\u001b[39;00m\n\u001b[32m    190\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map=collate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format.format(elem_type))\n",
            "\u001b[31mTypeError\u001b[39m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'numpy.datetime64'>"
          ]
        }
      ],
      "source": [
        "# Load and preprocess test data\n",
        "test_df = load_data('data/xauusd/1m/dynamic_labeled_test.csv')  # Replace with your test data file\n",
        "test_df = add_technical_indicators(test_df)\n",
        "test_df = add_basic_features(test_df)\n",
        "\n",
        "# Define feature columns\n",
        "feature_cols = [col for col in test_df.columns if col not in ['Label', 'Datetime']]\n",
        "\n",
        "# Create dataset and dataloader\n",
        "SEQ_LEN = 128\n",
        "BATCH_SIZE = 1  # Use batch size 1 for backtesting\n",
        "test_dataset = BacktestDataset(test_df, SEQ_LEN, feature_cols, use_time2vec=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Load model\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "N_FEATURES = len(feature_cols)\n",
        "model = EnhancedHybridModel(\n",
        "    num_features=N_FEATURES, \n",
        "    time_dim=1, \n",
        "    num_classes=3, \n",
        "    d_model=128, \n",
        "    nhead=8, \n",
        "    dim_feedforward=256, \n",
        "    num_layers=3\n",
        ")\n",
        "\n",
        "# Load trained model weights\n",
        "model.load_state_dict(torch.load('save/models/used_model.pth', map_location=DEVICE))\n",
        "\n",
        "# Run backtest\n",
        "strategy = run_backtest(model, test_loader, DEVICE)\n",
        "\n",
        "# Analyze results\n",
        "trades_df = strategy.get_trades_df()\n",
        "equity_df = strategy.get_equity_curve_df()\n",
        "\n",
        "# Plot results\n",
        "plot_equity_curve(equity_df)\n",
        "analyze_trades(trades_df)\n",
        "\n",
        "# Save results\n",
        "trades_df.to_csv('backtest_trades.csv', index=False)\n",
        "equity_df.to_csv('backtest_equity.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
